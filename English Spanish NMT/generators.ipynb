{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line_fit_tokenizer(line):\n",
    "    # Check if the line should be skipped\n",
    "    if ('(EC)' in line) or ('(EC)' in line):\n",
    "        return None\n",
    "\n",
    "    # Split the line by the tab character\n",
    "    segments = line.split('\\t')\n",
    "    segments = list(map(lambda x: x.lower(), segments))\n",
    "    if len(segments) >= 2:\n",
    "        # Extract English and Spanish segments\n",
    "        english = segments[0].strip()\n",
    "        spanish = segments[1].strip()\n",
    "\n",
    "        # Remove numbers, special characters and extra whitespaces\n",
    "        english = re.sub(r'[^A-Za-zñÑáéíóúÁÉÍÓÚüÜ]+', ' ', english).strip()\n",
    "        spanish = re.sub(r'[^A-Za-zñÑáéíóúÁÉÍÓÚüÜ]+', ' ', spanish).strip()\n",
    "\n",
    "        return english, spanish\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def process_line_getitem(line):\n",
    "    # Check if the line should be skipped\n",
    "    if ('(EC)' in line) or ('(EC)' in line):\n",
    "        return None, None\n",
    "\n",
    "    # Split the line by the tab character\n",
    "    segments = line.split('\\t')\n",
    "    segments = list(map(lambda x: x.lower(), segments))\n",
    "\n",
    "    if len(segments) >= 2:\n",
    "        # Extract English and Spanish segments\n",
    "        english = segments[0].strip()\n",
    "        spanish = segments[1].strip()\n",
    "\n",
    "        # Remove numbers, special characters and extra whitespaces\n",
    "        english = re.sub(r'[^A-Za-zñÑáéíóúÁÉÍÓÚüÜ]+', ' ', english).strip()\n",
    "        spanish = re.sub(r'[^A-Za-zñÑáéíóúÁÉÍÓÚüÜ]+', ' ', spanish).strip()\n",
    "\n",
    "        return english, spanish\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def reservoir_sampling(file, start, end, num_lines_to_use):\n",
    "    reservoir = [None] * num_lines_to_use\n",
    "    current_line_number = 0\n",
    "    \n",
    "    for line in file:\n",
    "        if current_line_number >= start and current_line_number < end:\n",
    "            i = current_line_number - start\n",
    "            if i < num_lines_to_use:\n",
    "                reservoir[i] = line\n",
    "            else:\n",
    "                j = np.random.randint(0, i + 1)\n",
    "                if j < num_lines_to_use:\n",
    "                    reservoir[j] = line\n",
    "        current_line_number += 1\n",
    "\n",
    "    return reservoir\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This data generator loads batch by batch. It is very ineficient. I would like to load a percentage and determine a maximum number of lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For selecting the data we use the reservoir sampling algorithm. It is used in the _fit_tokenizer method and\n",
    "#also in the __getitem__ method. \n",
    "'''\n",
    "Reservoir sampling is a randomized algorithm for choosing a simple random sample of k items from a list (or stream) containing n items, where n is either a very large or an unknown number. The reservoir sampling algorithm was introduced by Alan G. Waterman in 1978 and is particularly useful when n is large or the input list is in the form of a data stream that cannot be stored entirely in memory.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "    1. Create an empty reservoir (an array or list) of size k to store the selected items.\n",
    "    2. Fill the reservoir with the first k items of the input list.\n",
    "    3. For each item in the list after the k-th item (i.e., from item k+1 to item n), do the following:\n",
    "        a. Generate a random integer j between 0 (inclusive) and the current item's index (inclusive).\n",
    "        b. If j < k, replace the j-th item in the reservoir with the current item.\n",
    "    4. After processing all the items in the list, the reservoir will contain a simple random sample of k items.\n",
    "\n",
    "The reservoir sampling algorithm ensures that each item in the input list has an equal probability of being included in the final sample. The algorithm has a time complexity of O(n) and is memory-efficient, as it only needs to store k items at any given time.\n",
    "\n",
    "I\n",
    "\n",
    "''' \n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, file_path, batch_size=32, max_sequence_length=100, max_words=None, shuffle=True, tokenizer_data_percentage = .05, training_data_percentage = .05):\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.max_words = max_words\n",
    "        self.shuffle = shuffle\n",
    "        self.tokenizer_data_percentage = tokenizer_data_percentage\n",
    "        self.training_data_percentage = training_data_percentage\n",
    "\n",
    "        #Calculating total lines in the file:\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.total_lines = sum(1 for line in f)\n",
    "\n",
    "        self.selected_line_indices = self._reservoir_sampling_indices()\n",
    "\n",
    "        self.english_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, filters=\"\", lower=False)\n",
    "        self.spanish_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, filters=\"\", lower=False)\n",
    "\n",
    "        self._fit_tokenizers()\n",
    "\n",
    "    def _reservoir_sampling_indices(self):\n",
    "        num_lines_to_use = int(self.total_lines * self.training_data_percentage)\n",
    "        reservoir_indices = list(range(num_lines_to_use))\n",
    "\n",
    "        for i in range(num_lines_to_use, self.total_lines):\n",
    "            j = np.random.randint(0, i + 1)\n",
    "            if j < num_lines_to_use:\n",
    "                reservoir_indices[j] = i\n",
    "\n",
    "        return reservoir_indices\n",
    "    \n",
    "\n",
    "\n",
    "    def _fit_tokenizers(self):\n",
    "\n",
    "        num_lines_to_use = int(self.total_lines * self.tokenizer_data_percentage)\n",
    "        \n",
    "        selected_lines = [None] * num_lines_to_use\n",
    "\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            for line_idx, line in tqdm(enumerate(f), total = self.total_lines, desc = 'Reservoir sampling'):\n",
    "                if line_idx < num_lines_to_use:\n",
    "                    selected_lines[line_idx] = line\n",
    "                else:\n",
    "                    r = random.randint(0, line_idx)\n",
    "                    if r < num_lines_to_use:\n",
    "                        selected_lines[r] = line\n",
    "\n",
    "\n",
    "            #This is an alternative for loading the whole document and then sample\n",
    "            #lines = f.readlines()\n",
    "            #total_lines = len(lines)\n",
    "            #Randomly select 10% of the lines\n",
    "            #selected_lines = np.random.choice(lines, num_lines_to_use, replace = False)\n",
    "\n",
    "            #We will clean and store the line results\n",
    "        # Process the lines in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            processed_segments = list(tqdm(executor.map(process_line_fit_tokenizer, selected_lines), total=len(selected_lines), desc=\"Processing lines\"))\n",
    "\n",
    "        # Filter out None values and split the results into separate English and Spanish lists\n",
    "        english_segments = [x[0] for x in processed_segments if x is not None]\n",
    "        spanish_segments = [x[1] for x in processed_segments if x is not None]\n",
    "\n",
    "        # Fit the English and Spanish tokenizer with the cleaned selected lines\n",
    "        self.english_tokenizer.fit_on_texts(english_segments)\n",
    "        self.spanish_tokenizer.fit_on_texts(spanish_segments)\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            num_lines = sum(1 for line in f)\n",
    "            num_lines = int(num_lines * self.training_data_percentage) \n",
    "        return int(np.floor(num_lines / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        #self.selected_line_indices is the indices obtain from the reservoir sampling.\n",
    "        selected_line_indices = self.selected_line_indices[start:end]\n",
    "\n",
    "        selected_lines = []\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_index, line in enumerate(f):\n",
    "                if line_index in selected_line_indices:\n",
    "                    selected_lines.append(line)\n",
    "                    selected_line_indices.remove(line_index)\n",
    "                    if not selected_line_indices:\n",
    "                        break\n",
    "\n",
    "            # Process the selected lines using a ThreadPoolExecutor\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(executor.map(process_line_getitem, selected_lines))\n",
    "\n",
    "            english_batch, spanish_batch = [], []\n",
    "\n",
    "            # Results is a tuple with an English list and a Spanish list\n",
    "            for eng, spa in results:\n",
    "                if eng is not None and spa is not None:\n",
    "                    english_batch.append(eng)\n",
    "                    spanish_batch.append(spa)\n",
    "\n",
    "                    if len(english_batch) >= self.batch_size:\n",
    "                        break\n",
    "\n",
    "            english_sequences = self.english_tokenizer.texts_to_sequences(english_batch)\n",
    "            spanish_sequences = self.spanish_tokenizer.texts_to_sequences(spanish_batch)\n",
    "\n",
    "            padded_english_sequences = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, padding=\"post\")\n",
    "            padded_spanish_sequences = tf.keras.preprocessing.sequence.pad_sequences(spanish_sequences, padding=\"post\")\n",
    "\n",
    "            # The categorical target data must be used with a 'categorical_crossentropy' loss function.\n",
    "            #target_data = tf.keras.utils.to_categorical(padded_spanish_sequences, num_classes=self.max_words + 1)\n",
    "\n",
    "            return [padded_english_sequences, padded_spanish_sequences[:, :-1]], padded_spanish_sequences[:, 1:]\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            np.random.shuffle(lines)\n",
    "            with open(self.file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For selecting the data we use the reservoir sampling algorithm. It is used in the _fit_tokenizer method and\n",
    "#also in the __getitem__ method. \n",
    "'''\n",
    "Reservoir sampling is a randomized algorithm for choosing a simple random sample of k items from a list (or stream) containing n items, where n is either a very large or an unknown number. The reservoir sampling algorithm was introduced by Alan G. Waterman in 1978 and is particularly useful when n is large or the input list is in the form of a data stream that cannot be stored entirely in memory.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "    1. Create an empty reservoir (an array or list) of size k to store the selected items.\n",
    "    2. Fill the reservoir with the first k items of the input list.\n",
    "    3. For each item in the list after the k-th item (i.e., from item k+1 to item n), do the following:\n",
    "        a. Generate a random integer j between 0 (inclusive) and the current item's index (inclusive).\n",
    "        b. If j < k, replace the j-th item in the reservoir with the current item.\n",
    "    4. After processing all the items in the list, the reservoir will contain a simple random sample of k items.\n",
    "\n",
    "The reservoir sampling algorithm ensures that each item in the input list has an equal probability of being included in the final sample. The algorithm has a time complexity of O(n) and is memory-efficient, as it only needs to store k items at any given time.\n",
    "\n",
    "I\n",
    "\n",
    "''' \n",
    "\n",
    "class MemoryDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, file_path, batch_size=32, max_words=None, shuffle=True, tokenizer_data_percentage = .05, training_data_percentage = .05):\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.max_words = max_words\n",
    "        self.shuffle = shuffle\n",
    "        self.tokenizer_data_percentage = tokenizer_data_percentage\n",
    "        self.training_data_percentage = training_data_percentage\n",
    "\n",
    "        \n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.total_lines = sum(1 for line in f)\n",
    "\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            self.lines = reservoir_sampling(f, 0, self.total_lines, int(self.total_lines * self.training_data_percentage))\n",
    "        \n",
    "\n",
    "\n",
    "        self.english_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, filters=\"\", lower=False)\n",
    "        self.spanish_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, filters=\"\", lower=False)\n",
    "\n",
    "        self._fit_tokenizers()\n",
    "\n",
    "    def _reservoir_sampling_indices(self):\n",
    "        num_lines_to_use = int(self.total_lines * self.training_data_percentage)\n",
    "        reservoir_indices = list(range(num_lines_to_use))\n",
    "\n",
    "        for i in range(num_lines_to_use, self.total_lines):\n",
    "            j = np.random.randint(0, i + 1)\n",
    "            if j < num_lines_to_use:\n",
    "                reservoir_indices[j] = i\n",
    "\n",
    "        return reservoir_indices\n",
    "    \n",
    "\n",
    "\n",
    "    def _fit_tokenizers(self):\n",
    "\n",
    "        num_lines_to_use = int(self.total_lines * self.tokenizer_data_percentage)\n",
    "        \n",
    "        selected_lines = [None] * num_lines_to_use\n",
    "\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "            for line_idx, line in tqdm(enumerate(f), total = self.total_lines, desc = 'Reservoir sampling'):\n",
    "                if line_idx < num_lines_to_use:\n",
    "                    selected_lines[line_idx] = line\n",
    "                else:\n",
    "                    r = random.randint(0, line_idx)\n",
    "                    if r < num_lines_to_use:\n",
    "                        selected_lines[r] = line\n",
    "\n",
    "\n",
    "            #This is an alternative for loading the whole document and then sample\n",
    "            #lines = f.readlines()\n",
    "            #total_lines = len(lines)\n",
    "            #Randomly select 10% of the lines\n",
    "            #selected_lines = np.random.choice(lines, num_lines_to_use, replace = False)\n",
    "\n",
    "            #We will clean and store the line results\n",
    "        # Process the lines in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            processed_segments = list(tqdm(executor.map(process_line_fit_tokenizer, selected_lines), total=len(selected_lines), desc=\"Processing lines\"))\n",
    "\n",
    "        # Filter out None values and split the results into separate English and Spanish lists\n",
    "        english_segments = [x[0] for x in processed_segments if x is not None]\n",
    "        spanish_segments = [x[1] for x in processed_segments if x is not None]\n",
    "\n",
    "        # Fit the English and Spanish tokenizer with the cleaned selected lines\n",
    "        self.english_tokenizer.fit_on_texts(english_segments)\n",
    "        self.spanish_tokenizer.fit_on_texts(spanish_segments)\n",
    "\n",
    "    def __len__(self):\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            num_lines = sum(1 for line in f)\n",
    "            num_lines = int(num_lines * self.training_data_percentage) \n",
    "        return int(np.floor(num_lines / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        start = index * self.batch_size\n",
    "        end = (index + 1) * self.batch_size\n",
    "        #self.selected_line_indices is the indices obtain from the reservoir sampling.\n",
    "        selected_lines = self.lines[start:end]\n",
    "\n",
    "        # Process the selected lines using a ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(process_line_getitem, selected_lines))\n",
    "\n",
    "        english_batch, spanish_batch = [], []\n",
    "\n",
    "        # Results is a tuple with an English list and a Spanish list\n",
    "        for eng, spa in results:\n",
    "            if eng is not None and spa is not None:\n",
    "                english_batch.append(eng)\n",
    "                spanish_batch.append(spa)\n",
    "\n",
    "                if len(english_batch) >= self.batch_size:\n",
    "                    break\n",
    "\n",
    "        english_sequences = self.english_tokenizer.texts_to_sequences(english_batch)\n",
    "        spanish_sequences = self.spanish_tokenizer.texts_to_sequences(spanish_batch)\n",
    "\n",
    "        padded_english_sequences = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, padding=\"post\")\n",
    "        padded_spanish_sequences = tf.keras.preprocessing.sequence.pad_sequences(spanish_sequences, padding=\"post\")\n",
    "\n",
    "        # The categorical target data must be used with a 'categorical_crossentropy' loss function.\n",
    "        #target_data = tf.keras.utils.to_categorical(padded_spanish_sequences, num_classes=self.max_words + 1)\n",
    "\n",
    "        return [padded_english_sequences, padded_spanish_sequences[:, :-1]], padded_spanish_sequences[:, 1:]\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            np.random.shuffle(lines)\n",
    "            with open(self.file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, english_sentence, data_generator):\n",
    "    # Tokenize the English sentence\n",
    "    english_sequence = data_generator.english_tokenizer.texts_to_sequences([english_sentence])\n",
    "    padded_english_sequence = tf.keras.preprocessing.sequence.pad_sequences(english_sequence, padding=\"post\")\n",
    "\n",
    "    # Initialize the Spanish sequence with the <START> token\n",
    "    start_token_index = data_generator.spanish_tokenizer.word_index['<START>']\n",
    "    spanish_sequence = np.zeros((1, 1), dtype=np.int32)\n",
    "    spanish_sequence[0, 0] = start_token_index\n",
    "\n",
    "    spanish_tokens = []\n",
    "\n",
    "    # Generate the Spanish translation one token at a time\n",
    "    while True:\n",
    "        # Get the model's prediction\n",
    "        output_tokens = model.predict([padded_english_sequence, spanish_sequence])\n",
    "\n",
    "        # Choose the most probable token\n",
    "        predicted_token_index = np.argmax(output_tokens[0, -1])\n",
    "\n",
    "        # Stop if the <END> token is predicted or the maximum number of tokens is reached\n",
    "        if predicted_token_index == data_generator.spanish_tokenizer.word_index['<END>'] or len(spanish_tokens) >= 100:\n",
    "            break\n",
    "\n",
    "        # Append the predicted token to the current Spanish sequence\n",
    "        spanish_tokens.append(predicted_token_index)\n",
    "        spanish_sequence = np.hstack([spanish_sequence, np.array(predicted_token_index).reshape(1, 1)])\n",
    "\n",
    "    # Convert the Spanish tokens to a sentence\n",
    "    spanish_sentence = data_generator.spanish_tokenizer.sequences_to_texts([spanish_tokens])[0]\n",
    "    return spanish_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = '.\\EN-ES.txt\\EN-ES.txt'\n",
    "\n",
    "#batch_size = 32\n",
    "#max_sequence_length = 50\n",
    "#max_words = 1000\n",
    "\n",
    "#data_gen = DataGenerator(file_path, batch_size=batch_size, max_sequence_length=max_sequence_length, max_words=max_words, tokenizer_data_percentage= .01, training_data_percentage = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_batch = data_gen.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(first_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

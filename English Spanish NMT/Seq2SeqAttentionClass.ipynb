{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class Seq2SeqAttention(Model):\n",
    "    def __init__(self, vocab_size_source, vocab_size_target, embedding_dim, lstm_units):\n",
    "        super(Seq2SeqAttention, self).__init__()\n",
    "        #Define all the layers as attributes.\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_inputs = Input(shape=(None,))\n",
    "        self.encoder_embeddings = Embedding(vocab_size_source, embedding_dim)\n",
    "        self.encoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_inputs = Input(shape=(None,))\n",
    "        self.decoder_embeddings = Embedding(vocab_size_target, embedding_dim)\n",
    "        self.decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention_layer = Attention()\n",
    "\n",
    "        # Concatenate layer\n",
    "        self.decoder_concat = Concatenate(axis=-1)\n",
    "\n",
    "        # Dense layer for word prediction\n",
    "        self.word_prediction_layer = Dense(vocab_size_target, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #Link the layers.\n",
    "        encoder_input, decoder_input = inputs\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_embeddings = self.encoder_embeddings(encoder_input)\n",
    "        encoder_outputs, state_h, state_c = self.encoder_lstm(encoder_embeddings)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Decoder\n",
    "        decoder_embeddings = self.decoder_embeddings(decoder_input)\n",
    "        decoder_outputs, _, _ = self.decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention_output = self.attention_layer([decoder_outputs, encoder_outputs])\n",
    "        decoder_concat_input = self.decoder_concat([decoder_outputs, attention_output])\n",
    "\n",
    "        # Dense layer for word prediction\n",
    "        decoder_output = self.word_prediction_layer(decoder_concat_input)\n",
    "\n",
    "        return decoder_output\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
